STATEMENT OF QUALIFICATION Steven “Steve” Archuleta 
 
Beyond EDA (exploratory data analysis), I utilize Python-based integrated development environments to reveal central tendencies, correlations (bivariate and multivariate), and create data visualizations such as charts, graphs, and cross-tabular/contingency tables. I can also build machine learning models and deep neural networks for regression or classification problems. After meticulously cleaning raw data using munging/wrangling techniques, I have the ability to expose distribution patterns which may reveal highly skewed data. In such cases – provided no values are zero or negative – I can employ a log transformation to reduce the scale of the data and potentially alter its shape. If there's a risk of converting zeros or negatives into missing values, and the dataset requires a predefined redistribution range of 0 to 1 (or -1 to 1), I can employ a min-max normalization algorithm to shrink the data without changing the shape of the original distribution. Conversely, if a clean dataset is already normally distributed, I can use a standard scaling technique to redistribute the data into a predetermined range with a mean of zero and a standard deviation of one. This preprocessing in regression models scales the target value, which aids the model's learning. Lastly, if outliers have not been adequately addressed, a robust scaler method is available in scikit-learn. 
From the differentiation and classification of agricultural plant seedlings from invasive weeds (computer vision) to predicting loan defaults by customers, from estimating real estate sales prices in a given market to predicting used automobiles' mpg, I have identified problem sets, formulated hypotheses, collected, merged, and cleaned data, analyzed it, and converted raw data into useful business insights. These insights and recommendations are presented in IPYNB files via Jupyter Notebook and Google Colab. My AI/ML analysis of hidden information within structured data involves splitting data into training, validation, and testing datasets, imputing null values, scaling, normalizing, and transforming numeric values, encoding categorical values, and employing various machine learning algorithms to extrapolate predictions and/or classes. Prior to building a machine learning model, my data analysis may require Principal Component Analysis, duplicate data detection, missing value handling, outlier management, and assessment of unique values. I am also familiar with various metrics needed to measure the accuracy of residuals/errors/loss across models (regression: RMSE, MSE; classification: categorical cross entropy). The artificial intelligence and deep learning models that I work with require knowledge of hyperparameter tuning to avoid overfitting and underfitting. In neural networks, statistical information is input into models that use gradient descent via backpropagation to iteratively update parameters (weights and biases) until the loss function has reached a global minima. For successful model training, hyperparameters need to be strategically tuned on a validation set, ensuring optimal definition of the number of hidden layers, number of neurons/nodes within each 
hidden layer, epochs, batch size, learning rate, optimizers, loss functions, dropout rate, regularization, and normalization. My coding is primarily in Python via Pandas in Jupyter Notebook. 
Having gained certification in SQL and Database Architecture and Modeling through MySQL, I am proficient in data extraction, filtering, aggregation, and transformation, as well as joining and manipulating datasets. I can derive business insights through the incorporation of subqueries and windows functions. I am capable of designing Data Pipelines that use ingestion layers, transactional layers, and consumption layers, within which I create Tables, Procedures, Views, and Functions to properly define how data is stored, processed, and accessed. By studying problem definition and data dictionary, I can develop a solution architecture that enables me to write data definition languages, constraints, and keys, as well as normalize logic into fact tables and dimension tables. I create stored procedures and virtual view tables, and the modular way in which I calculate metrics makes it easy to handle changes in logic. Concurrently with MySQL, I use Python via Pandas in Jupyter Notebook to further verify the accuracy of my Database Architecture and Modeling. 
This April 2024, I completed a 12-week certification program in Generative AI for 
Natural Language Processing, wherein I ranked #1 of 30 postgraduate students and built a Data 
Science Assistant using OpenAI’s Chat GPT-4 API with LangChain Agents and the RAG (Retrieval Augmented Generation) prompt engineering technique; this showcased my skills in prompt engineering and the understanding of Transformer Neural Network Architecture with Large Language Models. During my three-year tenure as a student assistant for Unit A in the Information Technology Liaison Bureau within the Department of Social Services, I helped build internal and external websites and remediate PDF and MS Word files; I also wrote a 19-page business case that compared AWS Connect features and costs with Verizon InContact features and costs. In June 2019, I completed a 6-month MERN Stack boot camp at the University of California at Davis where I used HTML, CSS, JavaScript, jQuery, React, Node, Express.js, SQL, MongoDB, and many JS libraries/frameworks to build dynamic apps. I earned an A- average at UC Davis. In April 2020, I completed an 80-hour online OOP Java curriculum with the Revature Spark Program. In December 2020, I was one of twenty Sacramentans awarded a paid fellowship to study a 400-hour live Advanced Data Analytics Immersive taught by General Assembly, sponsored by the Greater Sacramento Urban League and the Sacramento Economic Council; therein, I utilized advanced Excel functions and pivot tables, SQL via PostgreSQL, Tableau for data visualization, and Python via Pandas in Jupyter Notebook. In January 2022, I graduated 36th of 176 postgraduate students in an Artificial Intelligence and Machine Learning program at the University of Texas, Austin. I completed the rigorous year-long modularized coursework with 802/860 marks for a 93.26% average. My nine formal projects covered supervised learning, unsupervised learning, linear regression, logistic regression, decision trees, ensemble techniques, feature engineering, model tuning, neural networks, computer vision, and natural language processing; I graduated with a 3.92 GPA, and my e-portfolio can be found here: https://eportfolio.mygreatlearning.com/steven-archuleta. On 27 May 2022, I completed an Introduction to Statistics Course at the College of Alameda, scoring 634.5 points out of 647 total points (98.07%), and in December 2022, I successfully completed an intensive 6-week SQL & Database Architecture and Modeling certification course with Great Learning. An assiduously committed lifelong learner, I am currently enrolled in a 2-year MScFE program, which also applies my acquired technological skillset and promises to further enhance my understanding of how to identify patterns and trends in data. At midlife, I’ve transformed myself; please hire me.

